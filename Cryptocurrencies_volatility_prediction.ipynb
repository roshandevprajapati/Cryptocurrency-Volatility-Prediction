{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Zb6q31ev1V"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import os\n",
        "import glob\n",
        "from numpy.random import seed\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from joblib import Parallel, delayed\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, SimpleRNN, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
        "from keras import Model\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Fix random seeds for reproducibility\n",
        "seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK3llmL4J-LN"
      },
      "source": [
        "# Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "r5VC6lecfIiH",
        "outputId": "d8dff134-8a0a-4f1a-8a12-aced2429cca0"
      },
      "outputs": [],
      "source": [
        "# 4 most traded cryptocurrencies in the past year\n",
        "cryptos = ['BITCOIN (BTC)', 'TRON (TRX)', 'Cardano (ADA)', 'Binance Coin (BNB)']\n",
        "\n",
        "all_data = {}\n",
        "for crypto in cryptos:\n",
        "    try:\n",
        "        # Dynamically create ticker symbol from the crypto name\n",
        "        ticker = crypto[-4:-1] + '-USD'\n",
        "        logging.info(f\"Downloading data for {crypto} with ticker {ticker}\")\n",
        "        t = yf.Ticker(ticker)\n",
        "        t_data = t.history(start=\"2018-01-01\", end=\"2024-01-01\", interval=\"1d\")\n",
        "        \n",
        "        if t_data.empty:\n",
        "            logging.warning(f\"No data found for {crypto}. Skipping.\")\n",
        "            continue\n",
        "        \n",
        "        t_data.index = t_data.index.date\n",
        "        t_data.drop(columns=['Dividends', 'Stock Splits'], inplace=True)\n",
        "        all_data[crypto] = t_data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error downloading data for {crypto}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqF77ROY5NH5"
      },
      "source": [
        "To evaluate the models’ performances, we need to compare predicted values of volatility with its actual values, however, volatility is an unobservable variable, and it is not measurable. Since standard deviation indicates how much a cryptocurrency has varied during a certain period, we can use this feature as a proxy for volatility. Therefore, historical volatility at day t is calculated as standard deviations of the returns’ values using the following equation where:\n",
        "\n",
        "\n",
        "*   r_i: the return value of cryptocurrency on day i\n",
        "*   (r_t)_: the average return value of cryptocurrency during past T trading days\n",
        "*   T=30 (approximately one month of transactions) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABjRBLV5gtU"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update: Code incorporates trading volume and technical indicators like the Moving Average Convergence Divergence (MACD) and Relative Strength Index (RSI)\n",
        "* Moving average convergence/divergence (MACD) is a technical indicator to help investors identify market entry points for buying or selling. The MACD line is calculated by subtracting the 26-period exponential moving average (EMA) from the 12-period EMA. The signal line is a nine-period EMA of the MACD line.\n",
        "* The relative strength index (RSI) is a momentum indicator used in technical analysis. RSI measures the speed and magnitude of a security's recent price changes to evaluate overvalued or undervalued conditions in the price of that security.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$RSI = 100 - \\frac{100}{1 + \\frac{AverageGain}{AverageLoss}}$$\n",
        "The average gain or loss used in this calculation is the average percentage gain or loss during a look-back period which is 14 days (2 weeks) in our code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "MauQ1QIxgMoO",
        "outputId": "8ec1096f-fcc6-4663-939d-93e511286e5e"
      },
      "outputs": [],
      "source": [
        "# Function to calculate MACD\n",
        "def calculate_macd(data):\n",
        "    short_ema = data['Close'].ewm(span=12, adjust=False).mean()\n",
        "    long_ema = data['Close'].ewm(span=26, adjust=False).mean()\n",
        "    macd = short_ema - long_ema\n",
        "    macd_signal = macd.ewm(span=9, adjust=False).mean()\n",
        "    return macd, macd_signal\n",
        "\n",
        "# Function to calculate RSI\n",
        "def calculate_rsi(data, period=14):\n",
        "    delta = data['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "# Correct the feature engineering part where MACD, MACD signal, and RSI are calculated.\n",
        "for crypto in all_data.keys(): \n",
        "    try:\n",
        "        # Calculate log-returns: log_ret = log(Close_t) - log(Close_(t-1))\n",
        "        log_ret = np.log(all_data[crypto].Close) - np.log(all_data[crypto].Close.shift(1)) \n",
        "        all_data[crypto]['returns'] = log_ret\n",
        "        \n",
        "        # Calculate Historical Volatilities\n",
        "        all_data[crypto]['HV'] = np.nan\n",
        "        for i in range(30, len(all_data[crypto])):\n",
        "            sample = all_data[crypto]['returns'][i-30:i].tolist()\n",
        "            all_data[crypto].loc[all_data[crypto].index[i], 'HV'] = statistics.stdev(sample)\n",
        "\n",
        "        # Calculate MACD and RSI correctly with all_data[crypto]\n",
        "        all_data[crypto]['MACD'], all_data[crypto]['MACD_signal'] = calculate_macd(all_data[crypto])\n",
        "        all_data[crypto]['RSI'] = calculate_rsi(all_data[crypto])\n",
        "        \n",
        "        # Handle infinite values and drop NaNs\n",
        "        all_data[crypto]['returns'].replace([np.inf, -np.inf], 0.000001, inplace=True)\n",
        "        all_data[crypto].dropna(inplace=True)  # The first 30 values of HV will be NaNs\n",
        "        all_data[crypto].drop(['Close', 'returns'], axis=1, inplace=True)  # Drop unnecessary columns\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing data for {crypto}: {e}\") \n",
        "\n",
        "## See an example:\n",
        "all_data['BITCOIN (BTC)']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AoPvIQbboISV",
        "outputId": "f950635e-1945-4673-a1a6-4df23d6136de"
      },
      "outputs": [],
      "source": [
        "# Line plots of Historical Volatility for each cryptocurrency\n",
        "for key, value in all_data.items():\n",
        "    try:\n",
        "        plt.figure(figsize=(18, 6))\n",
        "        plt.plot(value['HV'].values, color='blue', label=key)\n",
        "        plt.title(f'{key} HV Over Time', fontsize=18)\n",
        "        plt.ylabel('HV')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error plotting HV for {key}: {e}\")\n",
        "\n",
        "# Line plots of MACD for each cryptocurrency\n",
        "for key, value in all_data.items():\n",
        "    try:\n",
        "        plt.figure(figsize=(18, 6))\n",
        "        plt.plot(value['MACD'].values, color='blue', label='MACD')\n",
        "        plt.plot(value['MACD_signal'].values, color='red', label='MACD Signal')\n",
        "        plt.title(f'{key} MACD and MACD Signal Over Time', fontsize=18)\n",
        "        plt.ylabel('MACD')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error plotting MACD for {key}: {e}\")\n",
        "\n",
        "# Line plots of RSI for each cryptocurrency\n",
        "for key, value in all_data.items():\n",
        "    try:\n",
        "        plt.figure(figsize=(18, 6))\n",
        "        plt.plot(value['RSI'].values, color='green', label='RSI')\n",
        "        plt.axhline(y=70, color='red', linestyle='--', label='Overbought')\n",
        "        plt.axhline(y=30, color='blue', linestyle='--', label='Oversold')\n",
        "        plt.title(f'{key} RSI Over Time', fontsize=18)\n",
        "        plt.ylabel('RSI')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error plotting RSI for {key}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZBRzupy6v3Z"
      },
      "source": [
        "Each dataset will be divided into two parts: the first 80% as the training sample, and the remaining 20% as the testing sample. Moreover, for the purpose of hyper-parameter tuning of deep learning models, the last 20% of the training sample will be used as the validation sample. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itRV3_HCozEL",
        "outputId": "2292ee48-6c1b-4f04-b777-082714ce7f0e"
      },
      "outputs": [],
      "source": [
        "#### data split parameters\n",
        "example_data = all_data['BITCOIN (BTC)']\n",
        "train_valid_size = int(0.8*len(example_data))\n",
        "valid_size = int(0.2*(0.8*len(example_data)))\n",
        "train_size = train_valid_size - valid_size \n",
        "test_size = len(example_data) - int(0.8*len(example_data))\n",
        "\n",
        "print(\"size of the train_valid set: \", train_valid_size)\n",
        "print(\"size of the train set: \", train_size)\n",
        "print(\"size of the validation set: \", valid_size)\n",
        "print(\"size of the test set: \", test_size)\n",
        "print('total: ', train_size + valid_size + test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAjWxGQW70iN"
      },
      "source": [
        "The `prepare_input()` function prepares training, validation, and testing samples for each cryptocurrency dataset. \n",
        "`Feature` specifies the input data and `h` defines how many lags of input data should be used in training set construction. In other words, the objective is to train the models such that h lagged values of HV_t are fed as an input vector to predict HV_t. \n",
        "In this notebook, `Feature = ['HV']` and `h = 6` (approximately one week of transactions) will be used. \n",
        "*   For the purpose of hyper-parameter tuning on validation set, use the function as `prepare_input(data, train_size, valid_size, feature, h)`\n",
        "*   For the purpose of evaluating the models on test set, use the function as `prepare_input(data, train_valid_size, test_size, feature, h)`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuYPJ4VSq20M"
      },
      "outputs": [],
      "source": [
        "def prepare_input(data, train_size, valid_size, features, h):\n",
        "    try:\n",
        "        for feature in features:\n",
        "            if feature not in data.columns:\n",
        "                raise ValueError(f\"Feature '{feature}' not found in data columns.\")\n",
        "        \n",
        "        if len(data) < train_size + valid_size + h:\n",
        "            raise ValueError(\"Not enough data points to create training and validation sets with the specified sizes and time-steps.\")\n",
        "        \n",
        "        train_set = data[features].iloc[:train_size].values\n",
        "        valid_set = data[features].iloc[train_size:train_size + valid_size].values\n",
        "        \n",
        "        # Feature Scaling\n",
        "        sc = MinMaxScaler(feature_range=(0, 1))\n",
        "        train_set_scaled = sc.fit_transform(train_set)\n",
        "        \n",
        "        # Creating a data structure with h time-steps and multiple features as output\n",
        "        X_train, y_train = [], []\n",
        "        for i in range(h, train_size):\n",
        "            X_train.append(train_set_scaled[i-h:i])\n",
        "            y_train.append(train_set_scaled[i, 0])  # Adjusted to get the correct feature\n",
        "        \n",
        "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "        \n",
        "        # Preparing the validation set for making predictions\n",
        "        dataset_train_valid = pd.concat((data[features].iloc[:train_size], data[features].iloc[train_size:train_size + valid_size]), \n",
        "                                        axis=0)\n",
        "        valid_h = dataset_train_valid[len(dataset_train_valid) - len(data[features].iloc[train_size:train_size + valid_size]) - h:].values\n",
        "        if len(valid_h) < h:\n",
        "            raise ValueError(\"Validation set is too short for the specified time-steps.\")\n",
        "        valid_h = sc.transform(valid_h)\n",
        "        \n",
        "        X_valid = []\n",
        "        for i in range(h, valid_h.shape[0]):\n",
        "            X_valid.append(valid_h[i-h:i])\n",
        "        X_valid = np.array(X_valid)\n",
        "        \n",
        "        return X_train, y_train, X_valid, data['HV'][train_size:train_size + valid_size], train_set_scaled\n",
        "    except ValueError as ve:\n",
        "        logging.error(f\"ValueError in prepare_input: {ve}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error preparing input data: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8boe_r82KKIT"
      },
      "source": [
        "# Create models and define necessary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw-PhVA7rHdT"
      },
      "outputs": [],
      "source": [
        "def create_single_model(model_name, layer_number, hidden_units, dense_units, Dropout_layer, input_shape, activation, dropout_rate=0.2):\n",
        "    \"\"\"\n",
        "    Creates a model using Keras Functional API based on specified parameters.\n",
        "    \n",
        "    Parameters:\n",
        "    - model_name: str, type of model ('RNN', 'LSTM', 'BiLSTM')\n",
        "    - layer_number: int, number of RNN/LSTM layers\n",
        "    - hidden_units: int, number of units in each layer\n",
        "    - dense_units: int, number of units in dense layer\n",
        "    - Dropout_layer: bool, whether to include dropout layers\n",
        "    - input_shape: tuple, shape of the input data\n",
        "    - activation: str, activation function to use in hidden layers\n",
        "    - dropout_rate: float, dropout rate (default is 0.2)\n",
        "\n",
        "    Returns:\n",
        "    - model: Keras Model object\n",
        "    \"\"\"\n",
        "    \n",
        "    hidden_units = int(hidden_units)  # Ensure hidden_units is an integer\n",
        "    \n",
        "    input_layer = Input(shape=input_shape)\n",
        "    \n",
        "    if model_name == \"RNN\":\n",
        "        x = input_layer\n",
        "        for _ in range(layer_number):\n",
        "            x = SimpleRNN(hidden_units, return_sequences=True if _ < layer_number - 1 else False, activation=activation)(x)\n",
        "    elif model_name == \"LSTM\":\n",
        "        x = input_layer\n",
        "        for _ in range(layer_number):\n",
        "            x = LSTM(hidden_units, return_sequences=True if _ < layer_number - 1 else False, activation=activation)(x)\n",
        "    elif model_name == \"BiLSTM\":\n",
        "        x = input_layer\n",
        "        for _ in range(layer_number):\n",
        "            x = Bidirectional(LSTM(hidden_units, return_sequences=True if _ < layer_number - 1 else False, activation=activation))(x)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_name. Choose from 'RNN', 'LSTM', or 'BiLSTM'.\")\n",
        "    \n",
        "    # Optionally add dropout\n",
        "    if Dropout_layer:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    \n",
        "    # Dense layers after RNN/LSTM layers\n",
        "    z = Dense(dense_units, activation=activation)(x)\n",
        "    \n",
        "    # Optionally add batch normalization\n",
        "    z = BatchNormalization()(z)\n",
        "    \n",
        "    # Final output layer\n",
        "    output_layer = Dense(1, activation='linear')(z)\n",
        "    \n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-dt45vCqMH"
      },
      "source": [
        "To evaluate each model’s performance, we will use the Coefficient of determination (R2_Score): Indicates how well the model's predictions match the actual values, with 1 meaning perfect predictions and 0 meaning the model is no better than a simple mean. \n",
        "The R-squared (coefficient of determination) is given by:\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\( $y_i$ \\) is the actual value,\n",
        "- \\( $\\hat{y}_i$ \\) is the predicted value,\n",
        "- \\( $\\bar{y}$ \\) is the mean of the actual values,\n",
        "- \\( n \\) is the number of observations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZG4uQa8sxUe"
      },
      "outputs": [],
      "source": [
        "# Model evaluation function with error handling\n",
        "def model_evaluation(Tuning, model_name, layer_number, hidden_units, dense_units, \n",
        "                     Dropout_layer, input_shape, activation, loss, optimizer, epochs, batch_size):\n",
        "    try:\n",
        "        model = create_single_model(model_name, layer_number, hidden_units, dense_units, \n",
        "                                    Dropout_layer, input_shape, activation)\n",
        "        model.compile(loss=loss, optimizer=optimizer)\n",
        "        model_fit = model.fit(x=inputs[0], y=inputs[1], epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predicted_HV = model.predict(inputs[2])\n",
        "        \n",
        "        # Correct the reference to 'HV' in the model_evaluation() function\n",
        "        sc2 = MinMaxScaler(feature_range=(0, 1))\n",
        "        just_to_fit = data['HV'].iloc[train_size:train_valid_size].values if Tuning else data['HV'].iloc[train_valid_size:train_valid_size + test_size].values\n",
        "        just_to_fit = just_to_fit.reshape(len(just_to_fit), 1)\n",
        "        just_to_inverse = sc2.fit_transform(just_to_fit)\n",
        "\n",
        "        predicted_HV = sc2.inverse_transform(predicted_HV)\n",
        "        predicted_HV = predicted_HV.reshape(-1)\n",
        "        R2_Score = r2_score(inputs[3]['HV'].values, predicted_HV)\n",
        "        return R2_Score, predicted_HV\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during model evaluation: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B5sXu3p1HsX"
      },
      "source": [
        "# Experiments: validating and testing the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEss8bO5_JzV"
      },
      "source": [
        "\n",
        "\n",
        "In order to find an approximate value for the `epochs`, some trial and error was done in the following code, and training for at most 10 epochs was found to be enough. However, a more exact value for this hyper-parameter can be found via hyper-parameter tuning.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "aCQjpuv6Jrov",
        "outputId": "415eff94-0c60-47b6-e353-595a6b26c6dc"
      },
      "outputs": [],
      "source": [
        "example_data = all_data['BITCOIN (BTC)']\n",
        "inputs = prepare_input(example_data, train_valid_size, test_size, ['HV', 'MACD', 'MACD_signal', 'RSI', 'Volume'], 6)\n",
        "model = create_single_model('RNN', 1, 50, 64, True, (6, 1), 'tanh')\n",
        "model.compile(loss='mse', optimizer='RMSprop')\n",
        "model_fit = model.fit(x=inputs[0], y=inputs[1], epochs = 5, batch_size = 32, verbose=1)        ## after some trial and error,  training for at most 5 epochs was found to be enough \n",
        "## Plot losses\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.title('Training loss plot')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(model_fit.history['loss'], color = 'orange', label = 'training loss')\n",
        "plt.legend()\n",
        "\n",
        "predicted_HV = model.predict(inputs[2]) \n",
        "\n",
        "# Inversion of Predicted Values to Original Scale\n",
        "sc2 = MinMaxScaler(feature_range=(0, 1))\n",
        "just_to_fit = data['HV'].iloc[:train_valid_size].values.reshape(-1, 1)  # Fit on entire training+validation data\n",
        "just_to_inverse = sc2.fit_transform(just_to_fit)\n",
        "predicted_HV = sc2.inverse_transform(predicted_HV)\n",
        "\n",
        "predicted_HV = predicted_HV.reshape(-1)\n",
        "R2_Score = r2_score(inputs[3]['HV'].values, predicted_HV)\n",
        "print(R2_Score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNTKZMyH9tS7"
      },
      "source": [
        "The most important hyper-parameters will be tuned using grid search method on the validation set. Consequently, the models’ structures vary from one cryptocurrency to another as each model yields a specific set of optimal hyper-parameters for each cryptocurrency. The results of tuning for each crypto-model combination will be saved as a CSV file on Google Drive folder. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "\n",
        "def tune_model(crypto, model_name, layer_number, hidden_units, dense_units, activation, batch_size):\n",
        "    R2_tun = model_evaluation(True, model_name, layer_number, hidden_units, dense_units, True, input_shape, activation, loss, optimizer, epochs, batch_size)[0]\n",
        "    return [crypto, model_name, layer_number, hidden_units, dense_units, activation, batch_size, R2_tun]\n",
        "\n",
        "### Hyper-parameter tuning\n",
        "\n",
        "layer_numbers = [1, 2, 3, 4]  \n",
        "hidden_units_set = [50, 75, 100, 150]  \n",
        "dense_units_set = [64, 96, 128, 160]  \n",
        "activations = ['tanh', 'relu', 'sigmoid']  \n",
        "batch_sizes = [16, 32, 64]  \n",
        "optimizer = 'RMSprop'                                      \n",
        "loss = 'mean_squared_error'\n",
        "epochs = 5\n",
        "feature = ['HV', 'MACD', 'MACD_signal', 'RSI', 'Volume']\n",
        "h = 14                                                          ## 2 week of transactions\n",
        "model_names = ['RNN', 'LSTM']\n",
        "\n",
        "## A folder on Google Drive for saving the results of hyper-parameter tuning\n",
        "os.chdir(\"C:\\\\Users\\\\LENOVO\\\\Downloads\\\\output_cvproject\")  \n",
        "\n",
        "# Parallel tuning\n",
        "for key, value in all_data.items():\n",
        "    data = value\n",
        "    crypto = key\n",
        "    inputs = prepare_input(data, train_size, valid_size, feature, h)\n",
        "    input_shape = (inputs[0].shape[1], inputs[0].shape[2])\n",
        "\n",
        "    for model_name in model_names:\n",
        "        combinations = [(crypto, model_name, ln, hu, du, act, bs) for ln in layer_numbers\n",
        "                        for hu in hidden_units_set for du in dense_units_set \n",
        "                        for act in activations for bs in batch_sizes]\n",
        "        \n",
        "        tun_results = Parallel(n_jobs=-1)(delayed(tune_model)(*params) for params in combinations)\n",
        "\n",
        "        tun_results_df = pd.DataFrame(tun_results, columns=[\"crypto\", \"model_name\", \"layer_number\", \n",
        "                                                            \"hidden_units\", \"dense_units\", \"activation\", \"batch_size\",\n",
        "                                                              \"R2_tun\"])\n",
        "        tun_results_df = tun_results_df.sort_values('R2_tun')\n",
        "        tun_results_df.to_csv(crypto + ' ' + model_name + '.csv')\n",
        "        print(crypto + ' ' + model_name + ' tuning ' + 'is done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFZ1AVbcBVJ2"
      },
      "source": [
        "Since the tuning results were sorted by R2 Score values, the first row of each file is related to the optimized parameters. We can load all csv files and use the first row of each file (indicate by` iloc[0]` in the code) as the optimized parameters in our testing section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh8mlkTkPVab",
        "outputId": "e0a74f3d-40c2-4039-a78c-894939de0285"
      },
      "outputs": [],
      "source": [
        "# Check for empty or invalid CSV files during processing\n",
        "for t in tun_filenames:\n",
        "    if os.path.getsize(t) > 0:  # Only read non-empty files\n",
        "        df = pd.read_csv(t)\n",
        "        if not df.empty and 'crypto' in df.columns and 'model_name' in df.columns:\n",
        "            tun_data[t.replace('.csv', '')] = df\n",
        "        else:\n",
        "            logging.warning(f\"File {t} is missing required columns or is empty.\")\n",
        "    else:\n",
        "        logging.warning(f\"File {t} is empty or could not be loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdcUEwwnW8i9",
        "outputId": "a431befe-b61c-4f8d-e5d7-21bd9adab9e8"
      },
      "outputs": [],
      "source": [
        "for key, value in sorted(tun_data.items()):\n",
        "  print(key, value, sep = '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "kPJv5EDSyzo2",
        "outputId": "db657dfa-8f25-4795-95a4-fe350732e0f4"
      },
      "outputs": [],
      "source": [
        "seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# Initialize lists to store R2_Score results and predicted values\n",
        "R2_results = []\n",
        "Predicted_values = []\n",
        "\n",
        "# Loop through each cryptocurrency's tuning data\n",
        "for key, value in sorted(tun_data.items()):\n",
        "    try:\n",
        "        # Extract relevant information\n",
        "        crypto = tun_data[key].iloc[0]['crypto']\n",
        "        model_name = tun_data[key].iloc[0]['model_name']\n",
        "        print(f\"Evaluating model: {model_name} for crypto: {crypto}\")\n",
        "\n",
        "        # Prepare input data for the model\n",
        "        data = all_data[crypto]\n",
        "        inputs = prepare_input(data, train_valid_size, test_size, feature, h) \n",
        "        input_shape = (inputs[0].shape[1], inputs[0].shape[2])\n",
        "\n",
        "        # Perform model evaluation using the best hyperparameters from tuning\n",
        "        evaluation_result = model_evaluation(\n",
        "            Tuning=False,\n",
        "            model_name=model_name,\n",
        "            layer_number=tun_data[key].iloc[0]['layer_number'],\n",
        "            hidden_units=tun_data[key].iloc[0]['hidden_units'],\n",
        "            dense_units=tun_data[key].iloc[0]['dense_units'],\n",
        "            Dropout_layer=True,\n",
        "            input_shape=input_shape,\n",
        "            activation=tun_data[key].iloc[0]['activation'],\n",
        "            loss=loss,\n",
        "            optimizer=optimizer,\n",
        "            epochs=epochs,\n",
        "            batch_size=tun_data[key].iloc[0]['batch_size']\n",
        "        )\n",
        "\n",
        "        # Store the R2_Score result and predicted values\n",
        "        R2_results.append([crypto, model_name, evaluation_result[0]])\n",
        "        Predicted_values.append(evaluation_result[1])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {crypto} with model {model_name}: {e}\")\n",
        "\n",
        "# Create a DataFrame from R2_Score results for better visualization\n",
        "R2_results_df = pd.DataFrame(R2_results, columns=[\"crypto\", \"model_name\", \"R2_test\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(R2_results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wjgy3ia5ZazE",
        "outputId": "1cb36f79-6f25-4896-88d3-d3bfb79db1f0"
      },
      "outputs": [],
      "source": [
        "#### Plots of predicted vs. real HV\n",
        "for i in [0, 3, 6, 9]:\n",
        "  crypto = R2_results_df.iloc[i]['crypto']\n",
        "  data = all_data[crypto]\n",
        "  inputs = prepare_input(data, train_valid_size, test_size, feature, h) \n",
        "  plt.figure(figsize=(18,6))\n",
        "  plt.title(\"Predicted vs. real HV - \" + crypto + \" - test set\")\n",
        "  plt.plot( inputs[3]['HV'].values, color = 'gray', label = 'HV')\n",
        "  plt.plot(Predicted_values[i], color = 'red', label = R2_results_df.iloc[i]['model_name'] + '_HV')\n",
        "  plt.plot(Predicted_values[i+1], color = 'blue', label = R2_results_df.iloc[i+1]['model_name'] +'_HV')\n",
        "  plt.plot(Predicted_values[i+2], color = 'green', label = R2_results_df.iloc[i+2]['model_name'] +'_HV')\n",
        "\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('HV')\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure that 'i', 'i+1', and 'i+2' do not go out of bounds for 'Predicted_values' and 'R2_results_df'\n",
        "for i in range(0, len(R2_results_df), 3):\n",
        "    crypto = R2_results_df.iloc[i]['crypto']\n",
        "    data = all_data[crypto]\n",
        "    inputs = prepare_input(data, train_valid_size, test_size, feature, h) \n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    plt.title(f\"Predicted vs. Real HV - {crypto} - Test Set\")\n",
        "    plt.plot(inputs[3]['HV'].values, color='gray', label='Real HV')\n",
        "\n",
        "    # Plot up to three models' predictions if they exist\n",
        "    for j, color in zip(range(3), ['red', 'blue', 'green']):\n",
        "        if i + j < len(Predicted_values):\n",
        "            plt.plot(Predicted_values[i + j], color=color, label=f\"{R2_results_df.iloc[i + j]['model_name']}_HV\")\n",
        "\n",
        "    plt.xlabel('Day')\n",
        "    plt.ylabel('HV')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Cryptocurrencies volatility prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
